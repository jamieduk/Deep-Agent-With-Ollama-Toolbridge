# ===================================================
# OpenAI Tool Proxy - Environment Configuration
# ===================================================

# === General Settings ===
DEBUG_MODE=false  # Set to "true" to enable verbose logging

# === Backend Mode Configuration ===
# Controls which backend endpoint the proxy connects to (not the client request format)
# The proxy automatically detects client format (OpenAI or Ollama) and handles translation
# Values: "openai" or "ollama"
BACKEND_MODE=ollama #  Set to "ollama" or "openai" to use Ollama backend instead of OpenAI

# === OpenAI Backend Configuration ===
# Required when BACKEND_MODE=openai
# Base URL for your OpenAI-compatible backend (e.g., OpenAI API, Azure OpenAI, OpenRouter, local vLLM)
BACKEND_LLM_BASE_URL=YOUR_BACKEND_LLM_BASE_URL_HERE

# (Optional) Specific path for the chat completions endpoint if different from /v1/chat/completions
# BACKEND_LLM_CHAT_PATH=/v1/chat/completions

# (Optional) API Key for the OpenAI backend
# If not provided, the proxy will expect the client to send an 'Authorization: Bearer <key>' header
BACKEND_LLM_API_KEY=NA # YOUR_BACKEND_LLM_API_KEY_HERE

# === Ollama Backend Configuration ===
# Required when BACKEND_MODE=ollama
# Base URL for your Ollama instance
OLLAMA_BASE_URL=http://localhost:11434

# (Optional) API Key for Ollama, if your instance requires authentication
# Most local Ollama setups don't need this
OLLAMA_API_KEY=NA # OLLAMA_API_KEY_HERE

# (Optional) Default context length for Ollama synthetic /show responses
# Defaults to 32768 if not specified
OLLAMA_DEFAULT_CONTEXT_LENGTH=32768

# === Proxy Server Configuration ===
# Port for the proxy server to listen on, 11434 for Ollama
# (default: 4000)
PROXY_PORT=4000

# Host address to bind to (default: 0.0.0.0 - all interfaces)
# Use 127.0.0.1 to only allow connections from the local machine
PROXY_HOST=0.0.0.0

# === Optional Headers (for OpenRouter) ===
# Set these if you want openrouter stats/usage
HTTP_REFERER=YOUR_APP_URL_HERE
X_TITLE=YOUR_APP_NAME_HERE

# === Advanced Configuration ===
# Maximum buffer size for stream processing in bytes (default: 1MB)
# MAX_BUFFER_SIZE=1048576

# Connection timeout in milliseconds (default: 120000 - 2 minutes)
# CONNECTION_TIMEOUT=120000

# === Tool Instruction Reinjection Configuration ===
# Enable tool instruction reinjection (default: disabled)
# When enabled, tool instructions are periodically reminded to the model
ENABLE_TOOL_REINJECTION=true

# Number of tokens before reinjection (default: 3000)
TOOL_REINJECTION_TOKEN_COUNT=3000

# Number of messages before reinjection (default: 10)
TOOL_REINJECTION_MESSAGE_COUNT=10

# Type of reinjection: "full" or "reminder" (default: "full")
TOOL_REINJECTION_TYPE=full

# === Test Configuration ===
# Model to use for integration tests
TEST_MODEL=deepseek-ai/DeepSeek-R1
